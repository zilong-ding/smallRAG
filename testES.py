# smallrag_db.py
from datetime import datetime
from typing import Optional, Dict, Any, List
from elasticsearch import Elasticsearch, NotFoundError
from elasticsearch.exceptions import ConnectionError
import traceback

# =========================
# Elasticsearch æ•°æ®åº“ç®¡ç†ç±»
# =========================

class SmallRAGDB:
    def __init__(self, es_url: str = "http://localhost:9200"):
        self.es = Elasticsearch(es_url)
        self._indices = {
            "document": "smallrag_document_meta",
            "chunk": "smallrag_chunk_info",
            "qa": "smallrag_qa_history",
            "image": "smallrag_image_info"
        }

    # -------------------------
    # 1. ç´¢å¼•åˆå§‹åŒ–
    # -------------------------

    def init_indices(self, overwrite: bool = False) -> bool:
        """åˆå§‹åŒ–æ‰€æœ‰ç´¢å¼•"""
        try:
            if not self.es.ping():
                print("âŒ æ— æ³•è¿æ¥ Elasticsearch")
                return False

            mappings = self._get_mappings()

            for name, index_name in self._indices.items():
                if self.es.indices.exists(index=index_name):
                    if overwrite:
                        self.es.indices.delete(index=index_name)
                        self.es.indices.create(index=index_name, body=mappings[name])
                        print(f"ğŸ”„ å·²è¦†ç›–é‡å»ºç´¢å¼•: {index_name}")
                    else:
                        print(f"â„¹ï¸ ç´¢å¼•å·²å­˜åœ¨: {index_name}")
                else:
                    self.es.indices.create(index=index_name, body=mappings[name])
                    print(f"âœ… å·²åˆ›å»ºç´¢å¼•: {index_name}")

            print("ğŸ‰ æ‰€æœ‰ç´¢å¼•åˆå§‹åŒ–å®Œæˆï¼")
            return True

        except Exception as e:
            print("âŒ åˆå§‹åŒ–ç´¢å¼•å¤±è´¥:", str(e))
            traceback.print_exc()
            return False

    def _get_mappings(self) -> Dict[str, Dict]:
        """è¿”å›æ‰€æœ‰ç´¢å¼•çš„ mapping å®šä¹‰ï¼ˆä¸ä½ åŸå§‹å®šä¹‰ä¸€è‡´ï¼‰"""
        return {
            "document": {
                "mappings": {
                    "properties": {
                        "doc_id": {"type": "keyword"},
                        "workspace_id": {"type": "keyword"},
                        "user_username": {"type": "keyword"},
                        "title": {"type": "text", "analyzer": "ik_max_word", "search_analyzer": "ik_max_word"},
                        "file_name": {"type": "text", "analyzer": "ik_max_word", "search_analyzer": "ik_max_word"},
                        "abstract": {"type": "text", "analyzer": "ik_max_word", "search_analyzer": "ik_max_word"},
                        "full_content": {"type": "text", "analyzer": "ik_max_word", "search_analyzer": "ik_max_word"},
                        "embedding_status": {"type": "keyword"},
                        "file_size": {"type": "integer"},
                        "file_hash": {"type": "keyword"},
                        "created_at": {"type": "date"},
                        "updated_at": {"type": "date"}
                    }
                }
            },
            "chunk": {
                "mappings": {
                    "properties": {
                        "chunk_id": {"type": "keyword"},
                        "doc_id": {"type": "keyword"},
                        "workspace_id": {"type": "keyword"},
                        "user_username": {"type": "keyword"},
                        "chunk_content": {"type": "text", "analyzer": "ik_max_word", "search_analyzer": "ik_max_word"},
                        "embedding_vector": {"type": "dense_vector", "dims": 1536, "index": True, "similarity": "cosine"},
                        "chunk_order": {"type": "integer"},
                        "page_number": {"type": "integer"},
                        "metadata": {"type": "object"},
                        "created_at": {"type": "date"}
                    }
                }
            },
            "qa": {
                "mappings": {
                    "properties": {
                        "qa_id": {"type": "keyword"},
                        "user_username": {"type": "keyword"},
                        "question": {"type": "text", "analyzer": "ik_max_word", "search_analyzer": "ik_smart"},
                        "answer": {"type": "text", "analyzer": "ik_max_word", "search_analyzer": "ik_smart"},
                        "qa_vector": {"type": "dense_vector", "dims": 1536, "index": True, "similarity": "cosine"},
                        "qa_concat_vector": {"type": "dense_vector", "dims": 1536, "index": True, "similarity": "cosine"},
                        "workspace_id": {"type": "keyword"},
                        "created_at": {"type": "date"}
                    }
                }
            },
            "image": {
                "mappings": {
                    "properties": {
                        "image_id": {"type": "keyword"},
                        "user_username": {"type": "keyword"},
                        "workspace_id": {"type": "keyword"},
                        "image_path": {"type": "keyword"},
                        "caption": {"type": "text", "analyzer": "ik_max_word", "search_analyzer": "ik_smart"},
                        "tags": {"type": "keyword"},  # âœ… ç¡®ä¿æ˜¯ keyword
                        "embedding_vector": {"type": "dense_vector", "dims": 512, "index": True, "similarity": "cosine"},
                        "metadata": {"type": "object"},
                        "file_size": {"type": "integer"},
                        "width": {"type": "integer"},
                        "height": {"type": "integer"},
                        "format": {"type": "keyword"},
                        "created_at": {"type": "date"}
                    }
                }
            }
        }

    # -------------------------
    # 2. æ–‡æ¡£ï¼ˆDocumentï¼‰æ“ä½œ
    # -------------------------

    def create_document(self, doc_id: str, data: Dict[str, Any]) -> Dict:
        return self.es.index(index=self._indices["document"], id=doc_id, body=data)

    def get_document(self, doc_id: str) -> Optional[Dict]:
        try:
            return self.es.get(index=self._indices["document"], id=doc_id)["_source"]
        except NotFoundError:
            return None

    def update_document(self, doc_id: str, update_data: Dict[str, Any]) -> Dict:
        return self.es.update(index=self._indices["document"], id=doc_id, body={"doc": update_data})

    def delete_document(self, doc_id: str) -> Dict:
        return self.es.delete(index=self._indices["document"], id=doc_id)

    # -------------------------
    # 3. åˆ†å—ï¼ˆChunkï¼‰æ“ä½œ
    # -------------------------

    def create_chunk(self, chunk_id: str, data: Dict[str, Any]) -> Dict:
        return self.es.index(index=self._indices["chunk"], id=chunk_id, body=data)

    def get_chunk(self, chunk_id: str) -> Optional[Dict]:
        try:
            return self.es.get(index=self._indices["chunk"], id=chunk_id)["_source"]
        except NotFoundError:
            return None

    def update_chunk(self, chunk_id: str, update_data: Dict[str, Any]) -> Dict:
        return self.es.update(index=self._indices["chunk"], id=chunk_id, body={"doc": update_data})

    def delete_chunk(self, chunk_id: str) -> Dict:
        return self.es.delete(index=self._indices["chunk"], id=chunk_id)

    # -------------------------
    # 4. é—®ç­”ï¼ˆQAï¼‰æ“ä½œ
    # -------------------------

    def create_qa(self, qa_id: str, data: Dict[str, Any]) -> Dict:
        return self.es.index(index=self._indices["qa"], id=qa_id, body=data)

    def get_qa(self, qa_id: str) -> Optional[Dict]:
        try:
            return self.es.get(index=self._indices["qa"], id=qa_id)["_source"]
        except NotFoundError:
            return None

    def update_qa(self, qa_id: str, update_data: Dict[str, Any]) -> Dict:
        return self.es.update(index=self._indices["qa"], id=qa_id, body={"doc": update_data})

    def delete_qa(self, qa_id: str) -> Dict:
        return self.es.delete(index=self._indices["qa"], id=qa_id)

    # -------------------------
    # 5. å›¾ç‰‡ï¼ˆImageï¼‰æ“ä½œ
    # -------------------------

    def create_image(self, image_id: str, data: Dict[str, Any]) -> Dict:
        return self.es.index(index=self._indices["image"], id=image_id, body=data)

    def get_image(self, image_id: str) -> Optional[Dict]:
        try:
            return self.es.get(index=self._indices["image"], id=image_id)["_source"]
        except NotFoundError:
            return None

    def update_image(self, image_id: str, update_data: Dict[str, Any]) -> Dict:
        return self.es.update(index=self._indices["image"], id=image_id, body={"doc": update_data})

    def delete_image(self, image_id: str) -> Dict:
        return self.es.delete(index=self._indices["image"], id=image_id)

    # -------------------------
    # 6. æœç´¢æ¥å£
    # -------------------------

    def search_documents(self, query: Dict, size: int = 10) -> List[Dict]:
        res = self.es.search(index=self._indices["document"], body=query, size=size)
        return [hit["_source"] for hit in res["hits"]["hits"]]

    def search_chunks_by_vector(self, vector: List[float], k: int = 5) -> List[Dict]:
        res = self.es.search(
            index=self._indices["chunk"],
            body={
                "knn": {
                    "field": "embedding_vector",
                    "query_vector": vector,
                    "k": k,
                    "num_candidates": max(10, k * 2)
                }
            }
        )
        return [hit["_source"] for hit in res["hits"]["hits"]]

    def search_images_by_tags_and_caption(
        self,
        caption_keyword: str,
        tags: List[str],
        size: int = 10
    ) -> List[Dict]:
        query = {
            "query": {
                "bool": {
                    "must": [{"match": {"caption": caption_keyword}}],
                    "filter": [{"terms": {"tags": tags}}]
                }
            }
        }
        res = self.es.search(index=self._indices["image"], body=query, size=size)
        return [hit["_source"] for hit in res["hits"]["hits"]]

    def search_images_by_vector(self, vector: List[float], k: int = 5) -> List[Dict]:
        res = self.es.search(
            index=self._indices["image"],
            body={
                "knn": {
                    "field": "embedding_vector",
                    "query_vector": vector,
                    "k": k,
                    "num_candidates": max(10, k * 2)
                }
            }
        )
        return [hit["_source"] for hit in res["hits"]["hits"]]

    # -------------------------
    # 7. å·¥å…·æ–¹æ³•
    # -------------------------

    def refresh_all(self):
        """å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰ç´¢å¼•ï¼ˆæµ‹è¯•ç”¨ï¼‰"""
        for index in self._indices.values():
            self.es.indices.refresh(index=index)

if __name__ == "__main__":
    db = SmallRAGDB()

    # åˆå§‹åŒ–ï¼ˆæµ‹è¯•æ—¶å»ºè®® overwrite=Trueï¼‰
    db.init_indices(overwrite=True)

    # åˆ›å»ºæ–‡æ¡£
    doc_data = {
        "doc_id": "doc_1",
        "workspace_id": "ws_1",
        "user_username": "alice",
        "title": "æµ‹è¯•æŠ¥å‘Š",
        "full_content": "å†…å®¹...",
        "created_at": datetime.utcnow().isoformat(),
        "updated_at": datetime.utcnow().isoformat(),
        "embedding_status": "pending",
        "file_size": 1000,
        "file_hash": "xxx"
    }
    db.create_document("doc_1", doc_data)

    # æœç´¢å›¾ç‰‡
    images = db.search_images_by_tags_and_caption(caption_keyword="åŒ»ç–—", tags=["AI"])
    print("åŒ¹é…å›¾ç‰‡æ•°:", len(images))

    # å‘é‡æ£€ç´¢åˆ†å—ï¼ˆå‡è®¾ä½ æœ‰çœŸå®å‘é‡ï¼‰
    dummy_vec = [0.1] * 1536
    chunks = db.search_chunks_by_vector(dummy_vec, k=3)