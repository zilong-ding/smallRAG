import os
import time
from datetime import datetime, timezone
from typing import List
from testES import (  # å‡è®¾ä½ çš„æ¨¡å‹å’Œ DB ç±»åœ¨ smallrag_db.py
    SmallRAGDB,
    DocumentMeta,
    ChunkInfo,
    QAHistory,
    ImageInfo
)

# å¦‚æœä½ çš„ç±»å’Œæ¨¡å‹åœ¨åŒä¸€æ–‡ä»¶ï¼Œå¯æ”¹ä¸ºï¼š
# from smallrag_db import SmallRAGDB, DocumentMeta, ChunkInfo, QAHistory, ImageInfo

def now_utc():
    return datetime.now(timezone.utc)

def test_smallrag_db():
    es_url = os.getenv("ES_URL", "http://localhost:9200")
    db = SmallRAGDB(es_url=es_url)

    print("ğŸ§ª å¼€å§‹æµ‹è¯• SmallRAGDB...")

    # 1. åˆå§‹åŒ–ç´¢å¼•ï¼ˆè¦†ç›–ï¼‰
    print("\n1ï¸âƒ£ åˆå§‹åŒ–ç´¢å¼•...")
    assert db.init_indices(overwrite=True), "ç´¢å¼•åˆå§‹åŒ–å¤±è´¥"
    time.sleep(1)  # ç­‰å¾… ES å†…éƒ¨åˆ·æ–°

    # 2. æµ‹è¯• Document
    print("\n2ï¸âƒ£ æµ‹è¯• Document CRUD...")
    doc_id = "doc_001"
    doc_data = DocumentMeta(
        doc_id=doc_id,
        workspace_id="ws_123",
        user_username="alice",
        title="æµ‹è¯•æ–‡æ¡£",
        file_name="test.pdf",
        abstract="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ‘˜è¦",
        full_content="è¿™æ˜¯å®Œæ•´çš„æ–‡æ¡£å†…å®¹ï¼Œç”¨äºæµ‹è¯•ã€‚",
        file_size=1024,
        file_hash="abc123",
        created_at=now_utc(),
        updated_at=now_utc()
    )
    db.create_document(doc_id, doc_data)
    retrieved = db.get_document(doc_id)
    assert retrieved is not None
    assert retrieved["title"] == "æµ‹è¯•æ–‡æ¡£"
    print("âœ… Document åˆ›å»º & æŸ¥è¯¢æˆåŠŸ")

    # 3. æµ‹è¯• Chunkï¼ˆå«å‘é‡ï¼‰
    print("\n3ï¸âƒ£ æµ‹è¯• Chunk CRUD...")
    chunk_id = "chunk_001"
    query_vector = [0.9] + [0.1] * 1535  # å”¯ä¸€é«˜ç»´
    chunk_data = ChunkInfo(
        chunk_id=chunk_id,
        doc_id=doc_id,
        workspace_id="ws_123",
        user_username="alice",
        chunk_content="è¿™æ˜¯ç¬¬ä¸€ä¸ªåˆ†å—å†…å®¹ã€‚",
        embedding_vector=query_vector,
        chunk_order=1,
        page_number=1,
        metadata={"section": "introduction"},
        created_at=now_utc()
    )
    db.create_chunk(chunk_id, chunk_data)

    # 4. æ‰¹é‡åˆ›å»º Chunksï¼ˆä½¿ç”¨ä½ç›¸ä¼¼åº¦å‘é‡ï¼‰
    print("\n4ï¸âƒ£ æµ‹è¯•æ‰¹é‡åˆ›å»º Chunks...")
    chunks = []
    for i in range(2, 5):
        # ç¬¬ä¸€ä¸ªç»´åº¦æ•…æ„ä¸åŒï¼Œé™ä½ç›¸ä¼¼åº¦
        vec = [0.2] + [0.1] * 1535
        chunks.append(ChunkInfo(
            chunk_id=f"chunk_{i:03d}",
            doc_id=doc_id,
            workspace_id="ws_123",
            user_username="alice",
            chunk_content=f"è¿™æ˜¯ç¬¬ {i} ä¸ªåˆ†å—ã€‚",
            embedding_vector=vec,
            chunk_order=i,
            created_at=now_utc()
        ))
    db.bulk_create_chunks(chunks)
    db.refresh_all()
    time.sleep(2)  # ğŸ‘ˆ ç­‰å¾… ES æ„å»º ANN ç´¢å¼•

    # 5. æµ‹è¯• QA
    print("\n5ï¸âƒ£ æµ‹è¯• QA CRUD...")
    qa_id = "qa_001"
    fake_qa_vec = [0.5] * 1536
    qa_data = QAHistory(
        qa_id=qa_id,
        user_username="alice",
        question="è¿™æ˜¯ä»€ä¹ˆç³»ç»Ÿï¼Ÿ",
        answer="è¿™æ˜¯ä¸€ä¸ªå°å‹ RAG ç³»ç»Ÿã€‚",
        qa_vector=fake_qa_vec,
        qa_concat_vector=fake_qa_vec,
        workspace_id="ws_123",
        created_at=now_utc()
    )
    db.create_qa(qa_id, qa_data)
    qa_retrieved = db.get_qa(qa_id)
    assert qa_retrieved is not None
    assert "RAG" in qa_retrieved["answer"]
    print("âœ… QA åˆ›å»º & æŸ¥è¯¢æˆåŠŸ")

    # 6. æµ‹è¯• Imageï¼ˆå« tags å’Œå‘é‡ï¼‰
    print("\n6ï¸âƒ£ æµ‹è¯• Image CRUD...")
    image_id = "img_001"
    fake_img_vec = [0.3] * 512
    img_data = ImageInfo(
        image_id=image_id,
        user_username="alice",
        workspace_id="ws_123",
        image_path="/images/test.jpg",
        caption="ä¸€åªåœ¨è‰åœ°ä¸Šå¥”è·‘çš„ç‹—",
        tags=["dog", "outdoor", "animal"],
        embedding_vector=fake_img_vec,
        file_size=20480,
        width=800,
        height=600,
        format="JPEG",
        created_at=now_utc()
    )
    db.create_image(image_id, img_data)
    img_retrieved = db.get_image(image_id)
    assert img_retrieved is not None
    assert "dog" in img_retrieved["tags"]
    db.refresh_all()
    print("âœ… Image åˆ›å»º & æŸ¥è¯¢æˆåŠŸ")

    # 7. æµ‹è¯•å‘é‡æœç´¢
    print("\n7ï¸âƒ£ æµ‹è¯• Chunk å‘é‡æœç´¢...")
    results = db.search_chunks_by_vector(query_vector, k=2)
    assert len(results) >= 1
    assert results[0]["chunk_id"] == "chunk_001", f"Expected chunk_001, got {results[0]['chunk_id']}"
    print("âœ… Chunk å‘é‡æœç´¢æˆåŠŸ")

    # 8. æµ‹è¯•å›¾ç‰‡æ··åˆæœç´¢ï¼ˆcaption + tagsï¼‰
    print("\n8ï¸âƒ£ æµ‹è¯•å›¾ç‰‡æ··åˆæœç´¢...")
    img_results = db.search_images_by_tags_and_caption(
        caption_keyword="ç‹—",
        tags=["dog", "animal"],
        size=5
    )
    assert len(img_results) >= 1
    assert img_results[0]["image_id"] == "img_001"
    print("âœ… å›¾ç‰‡æ··åˆæœç´¢æˆåŠŸ")

    # 9. æµ‹è¯•ä¸å­˜åœ¨çš„ ID
    print("\n9ï¸âƒ£ æµ‹è¯•æŸ¥è¯¢ä¸å­˜åœ¨çš„æ–‡æ¡£...")
    none_doc = db.get_document("non_existent_doc")
    assert none_doc is None
    print("âœ… ä¸å­˜åœ¨æ–‡æ¡£è¿”å› Noneï¼Œç¬¦åˆé¢„æœŸ")

    # 10. éªŒè¯ tags å­—æ®µä¸º keywordï¼ˆå¯é€šè¿‡ mapping æ£€æŸ¥ï¼‰
    print("\nğŸ”Ÿ éªŒè¯ image.tags ä¸º keyword ç±»å‹...")
    mapping = db.es.indices.get_mapping(index=db._indices["image"])
    tags_type = mapping[db._indices["image"]]["mappings"]["properties"]["tags"]["type"]
    assert tags_type == "keyword", f"tags ç±»å‹åº”ä¸º keywordï¼Œå®é™…ä¸º {tags_type}"
    print("âœ… tags å­—æ®µç±»å‹æ­£ç¡®")

    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")


if __name__ == "__main__":
    test_smallrag_db()